import math
from typing import Dict, List, Optional, Union

import tiktoken
from openai import (
    APIError,
    AsyncAzureOpenAI,
    AsyncOpenAI,
    AuthenticationError,
    OpenAIError,
    RateLimitError,
)
from openai.types.chat import ChatCompletion, ChatCompletionMessage
from tenacity import (
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_random_exponential,
)

from app.bedrock import BedrockClient
from app.config import LLMSettings, config
from app.exceptions import TokenLimitExceeded
from app.logger import logger  # Assuming a logger is set up in your app
from app.schema import (
    ROLE_VALUES,
    TOOL_CHOICE_TYPE,
    TOOL_CHOICE_VALUES,
    Message,
    ToolChoice,
)


REASONING_MODELS = ["o1", "o3-mini"]
MULTIMODAL_MODELS = [
    "gpt-4-vision-preview",
    "gpt-4o",
    "gpt-4o-mini",
    "claude-3-opus-20240229",
    "claude-3-sonnet-20240229",
    "claude-3-haiku-20240307",
    "qwen-vl-plus",  # DashScope è§†è§‰æ¨¡å‹
    "qwen-vl-max",  # DashScope è§†è§‰æ¨¡å‹
    "qwen/qwen2.5-vl-72b-instruct",  # DashScope è§†è§‰æ¨¡å‹
]


class TokenCounter:
    # Token å¸¸é‡
    BASE_MESSAGE_TOKENS = 4
    FORMAT_TOKENS = 2
    LOW_DETAIL_IMAGE_TOKENS = 85
    HIGH_DETAIL_TILE_TOKENS = 170

    # å›¾åƒå¤„ç†å¸¸é‡
    MAX_SIZE = 2048
    HIGH_DETAIL_TARGET_SHORT_SIDE = 768
    TILE_SIZE = 512

    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def count_text(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬å­—ç¬¦ä¸²çš„ token æ•°"""
        return 0 if not text else len(self.tokenizer.encode(text))

    def count_image(self, image_item: dict) -> int:
        """
        æ ¹æ®ç»†èŠ‚çº§åˆ«å’Œå°ºå¯¸è®¡ç®—å›¾åƒçš„ token æ•°

        å¯¹äº "low" ç»†èŠ‚ï¼šå›ºå®š 85 tokens
        å¯¹äº "high" ç»†èŠ‚ï¼š
        1. ç¼©æ”¾åˆ°é€‚åˆ 2048x2048 æ­£æ–¹å½¢
        2. å°†æœ€çŸ­è¾¹ç¼©æ”¾åˆ° 768px
        3. è®¡ç®— 512px ç“¦ç‰‡æ•°é‡ï¼ˆæ¯ä¸ª 170 tokensï¼‰
        4. æ·»åŠ  85 tokens
        """
        detail = image_item.get("detail", "medium")

        # å¯¹äºä½ç»†èŠ‚ï¼Œå§‹ç»ˆè¿”å›å›ºå®š token æ•°
        if detail == "low":
            return self.LOW_DETAIL_IMAGE_TOKENS

        # å¯¹äºä¸­ç­‰ç»†èŠ‚ï¼ˆOpenAI ä¸­çš„é»˜è®¤å€¼ï¼‰ï¼Œä½¿ç”¨é«˜ç»†èŠ‚è®¡ç®—
        # OpenAI æ²¡æœ‰ä¸ºä¸­ç­‰ç»†èŠ‚æŒ‡å®šå•ç‹¬çš„è®¡ç®—æ–¹æ³•

        # å¯¹äºé«˜ç»†èŠ‚ï¼Œå¦‚æœå¯ç”¨ï¼Œåˆ™åŸºäºå°ºå¯¸è®¡ç®—
        if detail == "high" or detail == "medium":
            # å¦‚æœåœ¨ image_item ä¸­æä¾›äº†å°ºå¯¸
            if "dimensions" in image_item:
                width, height = image_item["dimensions"]
                return self._calculate_high_detail_tokens(width, height)

        return (
            self._calculate_high_detail_tokens(1024, 1024) if detail == "high" else 1024
        )

    def _calculate_high_detail_tokens(self, width: int, height: int) -> int:
        """æ ¹æ®å°ºå¯¸è®¡ç®—é«˜ç»†èŠ‚å›¾åƒçš„ token æ•°"""
        # æ­¥éª¤ 1ï¼šç¼©æ”¾åˆ°é€‚åˆ MAX_SIZE x MAX_SIZE æ­£æ–¹å½¢
        if width > self.MAX_SIZE or height > self.MAX_SIZE:
            scale = self.MAX_SIZE / max(width, height)
            width = int(width * scale)
            height = int(height * scale)

        # æ­¥éª¤ 2ï¼šç¼©æ”¾ä½¿æœ€çŸ­è¾¹ä¸º HIGH_DETAIL_TARGET_SHORT_SIDE
        scale = self.HIGH_DETAIL_TARGET_SHORT_SIDE / min(width, height)
        scaled_width = int(width * scale)
        scaled_height = int(height * scale)

        # æ­¥éª¤ 3ï¼šè®¡ç®— 512px ç“¦ç‰‡æ•°é‡
        tiles_x = math.ceil(scaled_width / self.TILE_SIZE)
        tiles_y = math.ceil(scaled_height / self.TILE_SIZE)
        total_tiles = tiles_x * tiles_y

        # æ­¥éª¤ 4ï¼šè®¡ç®—æœ€ç»ˆ token æ•°
        return (
            total_tiles * self.HIGH_DETAIL_TILE_TOKENS
        ) + self.LOW_DETAIL_IMAGE_TOKENS

    def count_content(self, content: Union[str, List[Union[str, dict]]]) -> int:
        """è®¡ç®—æ¶ˆæ¯å†…å®¹çš„ token æ•°"""
        if not content:
            return 0

        if isinstance(content, str):
            return self.count_text(content)

        token_count = 0
        for item in content:
            if isinstance(item, str):
                token_count += self.count_text(item)
            elif isinstance(item, dict):
                if "text" in item:
                    token_count += self.count_text(item["text"])
                elif "image_url" in item:
                    token_count += self.count_image(item)
        return token_count

    def count_tool_calls(self, tool_calls: List[dict]) -> int:
        """è®¡ç®—å·¥å…·è°ƒç”¨çš„ token æ•°"""
        token_count = 0
        for tool_call in tool_calls:
            if "function" in tool_call:
                function = tool_call["function"]
                token_count += self.count_text(function.get("name", ""))
                token_count += self.count_text(function.get("arguments", ""))
        return token_count

    def count_message_tokens(self, messages: List[dict]) -> int:
        """è®¡ç®—æ¶ˆæ¯åˆ—è¡¨ä¸­çš„ token æ€»æ•°"""
        total_tokens = self.FORMAT_TOKENS  # åŸºç¡€æ ¼å¼ tokens

        for message in messages:
            tokens = self.BASE_MESSAGE_TOKENS  # æ¯æ¡æ¶ˆæ¯çš„åŸºç¡€ tokens

            # æ·»åŠ è§’è‰² tokens
            tokens += self.count_text(message.get("role", ""))

            # æ·»åŠ å†…å®¹ tokens
            if "content" in message:
                tokens += self.count_content(message["content"])

            # æ·»åŠ å·¥å…·è°ƒç”¨ tokens
            if "tool_calls" in message:
                tokens += self.count_tool_calls(message["tool_calls"])

            # æ·»åŠ  name å’Œ tool_call_id tokens
            tokens += self.count_text(message.get("name", ""))
            tokens += self.count_text(message.get("tool_call_id", ""))

            total_tokens += tokens

        return total_tokens


class LLM:
    _instances: Dict[str, "LLM"] = {}

    def __new__(
        cls, config_name: str = "default", llm_config: Optional[LLMSettings] = None
    ):
        if config_name not in cls._instances:
            instance = super().__new__(cls)
            instance.__init__(config_name, llm_config)
            cls._instances[config_name] = instance
        return cls._instances[config_name]

    def __init__(
        self, config_name: str = "default", llm_config: Optional[LLMSettings] = None
    ):
        if not hasattr(self, "client"):  # ä»…åœ¨å°šæœªåˆå§‹åŒ–æ—¶åˆå§‹åŒ–
            llm_config = llm_config or config.llm
            llm_config = llm_config.get(config_name, llm_config["default"])
            self.model = llm_config.model
            self.max_tokens = llm_config.max_tokens
            self.temperature = llm_config.temperature
            self.api_type = llm_config.api_type
            self.api_key = llm_config.api_key
            self.api_version = llm_config.api_version
            self.base_url = llm_config.base_url

            # æ·»åŠ  token è®¡æ•°ç›¸å…³å±æ€§
            self.total_input_tokens = 0
            self.total_completion_tokens = 0
            self.max_input_tokens = (
                llm_config.max_input_tokens
                if hasattr(llm_config, "max_input_tokens")
                else None
            )

            # åˆå§‹åŒ– tokenizer
            try:
                self.tokenizer = tiktoken.encoding_for_model(self.model)
            except KeyError:
                # å¦‚æœæ¨¡å‹ä¸åœ¨ tiktoken çš„é¢„è®¾ä¸­ï¼Œä½¿ç”¨ cl100k_base ä½œä¸ºé»˜è®¤å€¼
                self.tokenizer = tiktoken.get_encoding("cl100k_base")

            if self.api_type == "azure":
                self.client = AsyncAzureOpenAI(
                    base_url=self.base_url,
                    api_key=self.api_key,
                    api_version=self.api_version,
                )
            elif self.api_type == "aws":
                self.client = BedrockClient()
            else:
                self.client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)

            self.token_counter = TokenCounter(self.tokenizer)

    def count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬ä¸­çš„ token æ•°"""
        if not text:
            return 0
        return len(self.tokenizer.encode(text))

    def count_message_tokens(self, messages: List[dict]) -> int:
        return self.token_counter.count_message_tokens(messages)

    def update_token_count(self, input_tokens: int, completion_tokens: int = 0) -> None:
        """æ›´æ–° token è®¡æ•°"""
        # ä»…åœ¨è®¾ç½®äº† max_input_tokens æ—¶è·Ÿè¸ª tokens
        self.total_input_tokens += input_tokens
        self.total_completion_tokens += completion_tokens
        logger.info(
            f"Token usage: Input={input_tokens}, Completion={completion_tokens}, "
            f"Cumulative Input={self.total_input_tokens}, Cumulative Completion={self.total_completion_tokens}, "
            f"Total={input_tokens + completion_tokens}, Cumulative Total={self.total_input_tokens + self.total_completion_tokens}"
        )

    def check_token_limit(self, input_tokens: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¶…è¿‡ token é™åˆ¶"""
        if self.max_input_tokens is not None:
            return (self.total_input_tokens + input_tokens) <= self.max_input_tokens
        # å¦‚æœæœªè®¾ç½® max_input_tokensï¼Œå§‹ç»ˆè¿”å› True
        return True

    def get_limit_error_message(self, input_tokens: int) -> str:
        """ç”Ÿæˆ token é™åˆ¶è¶…å‡ºçš„é”™è¯¯æ¶ˆæ¯"""
        if (
            self.max_input_tokens is not None
            and (self.total_input_tokens + input_tokens) > self.max_input_tokens
        ):
            return f"Request may exceed input token limit (Current: {self.total_input_tokens}, Needed: {input_tokens}, Max: {self.max_input_tokens})"

        return "Token limit exceeded"

    @staticmethod
    def format_messages(
        messages: List[Union[dict, Message]], supports_images: bool = False
    ) -> List[dict]:
        """
        é€šè¿‡å°†æ¶ˆæ¯è½¬æ¢ä¸º OpenAI æ¶ˆæ¯æ ¼å¼æ¥æ ¼å¼åŒ– LLM çš„æ¶ˆæ¯ã€‚

        Args:
            messages: å¯ä»¥æ˜¯ dict æˆ– Message å¯¹è±¡çš„æ¶ˆæ¯åˆ—è¡¨
            supports_images: æŒ‡ç¤ºç›®æ ‡æ¨¡å‹æ˜¯å¦æ”¯æŒå›¾åƒè¾“å…¥çš„æ ‡å¿—

        Returns:
            List[dict]: OpenAI æ ¼å¼çš„æ ¼å¼åŒ–æ¶ˆæ¯åˆ—è¡¨

        Raises:
            ValueError: å¦‚æœæ¶ˆæ¯æ— æ•ˆæˆ–ç¼ºå°‘å¿…éœ€å­—æ®µ
            TypeError: å¦‚æœæä¾›äº†ä¸æ”¯æŒçš„æ¶ˆæ¯ç±»å‹

        Examples:
            >>> msgs = [
            ...     Message.system_message("You are a helpful assistant"),
            ...     {"role": "user", "content": "Hello"},
            ...     Message.user_message("How are you?")
            ... ]
            >>> formatted = LLM.format_messages(msgs)
        """
        formatted_messages = []

        for message in messages:
            # å°† Message å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
            if isinstance(message, Message):
                message = message.to_dict()

            if isinstance(message, dict):
                # å¦‚æœæ¶ˆæ¯æ˜¯å­—å…¸ï¼Œç¡®ä¿å®ƒå…·æœ‰å¿…éœ€å­—æ®µ
                if "role" not in message:
                    raise ValueError("Message dict must contain 'role' field")

                # å¦‚æœå­˜åœ¨ base64 å›¾åƒä¸”æ¨¡å‹æ”¯æŒå›¾åƒï¼Œåˆ™å¤„ç†å®ƒä»¬
                if supports_images and message.get("base64_image"):
                    # åˆå§‹åŒ–æˆ–å°†å†…å®¹è½¬æ¢ä¸ºé€‚å½“æ ¼å¼
                    if not message.get("content"):
                        message["content"] = []
                    elif isinstance(message["content"], str):
                        message["content"] = [
                            {"type": "text", "text": message["content"]}
                        ]
                    elif isinstance(message["content"], list):
                        # å°†å­—ç¬¦ä¸²é¡¹è½¬æ¢ä¸ºé€‚å½“çš„æ–‡æœ¬å¯¹è±¡
                        message["content"] = [
                            (
                                {"type": "text", "text": item}
                                if isinstance(item, str)
                                else item
                            )
                            for item in message["content"]
                        ]

                    # å°†å›¾åƒæ·»åŠ åˆ°å†…å®¹ä¸­
                    message["content"].append(
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{message['base64_image']}"
                            },
                        }
                    )

                    # åˆ é™¤ base64_image å­—æ®µ
                    del message["base64_image"]
                # å¦‚æœæ¨¡å‹ä¸æ”¯æŒå›¾åƒä½†æ¶ˆæ¯æœ‰ base64_imageï¼Œåˆ™ä¼˜é›…å¤„ç†
                elif not supports_images and message.get("base64_image"):
                    # ä»…åˆ é™¤ base64_image å­—æ®µå¹¶ä¿ç•™æ–‡æœ¬å†…å®¹
                    del message["base64_image"]

                if "content" in message or "tool_calls" in message:
                    formatted_messages.append(message)
                # else: ä¸åŒ…å«è¯¥æ¶ˆæ¯
            else:
                raise TypeError(f"Unsupported message type: {type(message)}")

        # éªŒè¯æ‰€æœ‰æ¶ˆæ¯éƒ½æœ‰å¿…éœ€å­—æ®µ
        for msg in formatted_messages:
            if msg["role"] not in ROLE_VALUES:
                raise ValueError(f"Invalid role: {msg['role']}")

        return formatted_messages

    @retry(
        wait=wait_random_exponential(min=1, max=60),
        stop=stop_after_attempt(6),
        retry=retry_if_exception_type(
            (OpenAIError, Exception, ValueError)
        ),  # Don't retry TokenLimitExceeded
    )
    async def ask(
        self,
        messages: List[Union[dict, Message]],
        system_msgs: Optional[List[Union[dict, Message]]] = None,
        stream: bool = True,
        temperature: Optional[float] = None,
    ) -> str:
        """
        å‘ LLM å‘é€æç¤ºå¹¶è·å–å“åº”ã€‚

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            system_msgs: å¯é€‰çš„è¦å‰ç½®çš„ç³»ç»Ÿæ¶ˆæ¯
            stream (bool): æ˜¯å¦æµå¼ä¼ è¾“å“åº”
            temperature (float): å“åº”çš„é‡‡æ ·æ¸©åº¦

        Returns:
            str: ç”Ÿæˆçš„å“åº”

        Raises:
            TokenLimitExceeded: å¦‚æœè¶…è¿‡ token é™åˆ¶
            ValueError: å¦‚æœæ¶ˆæ¯æ— æ•ˆæˆ–å“åº”ä¸ºç©º
            OpenAIError: å¦‚æœ API è°ƒç”¨åœ¨é‡è¯•åå¤±è´¥
            Exception: å¯¹äºæ„å¤–é”™è¯¯
        """
        try:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒå›¾åƒ
            supports_images = self.model in MULTIMODAL_MODELS

            # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥æ˜¯å¦æœ‰å›¾åƒè¾“å…¥
            has_images = any(
                isinstance(msg, dict) and msg.get("base64_image")
                or isinstance(msg, Message) and msg.base64_image
                for msg in (system_msgs or []) + messages
            )

            if supports_images:
                logger.info(f"ğŸ‘ï¸ Vision model enabled: {self.model} (supports images)")
                if has_images:
                    logger.info(f"ğŸ“· Image detected in messages - will be sent to vision model")
                else:
                    logger.debug(f"ğŸ“· No image in current messages")
            else:
                logger.warning(f"âš ï¸ Model {self.model} does NOT support images - visual understanding disabled")
                if has_images:
                    logger.warning(f"âš ï¸ Images detected but will be ignored (model doesn't support vision)")

            # ä½¿ç”¨å›¾åƒæ”¯æŒæ£€æŸ¥æ ¼å¼åŒ–ç³»ç»Ÿå’Œç”¨æˆ·æ¶ˆæ¯
            if system_msgs:
                system_msgs = self.format_messages(system_msgs, supports_images)
                messages = system_msgs + self.format_messages(messages, supports_images)
            else:
                messages = self.format_messages(messages, supports_images)

            # è®¡ç®—è¾“å…¥ token æ•°
            input_tokens = self.count_message_tokens(messages)

            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡ token é™åˆ¶
            if not self.check_token_limit(input_tokens):
                error_message = self.get_limit_error_message(input_tokens)
                # å¼•å‘ä¸€ä¸ªä¸ä¼šè¢«é‡è¯•çš„ç‰¹æ®Šå¼‚å¸¸
                raise TokenLimitExceeded(error_message)

            params = {
                "model": self.model,
                "messages": messages,
            }

            if self.model in REASONING_MODELS:
                params["max_completion_tokens"] = self.max_tokens
            else:
                params["max_tokens"] = self.max_tokens
                params["temperature"] = (
                    temperature if temperature is not None else self.temperature
                )

            if not stream:
                # éæµå¼è¯·æ±‚
                response = await self.client.chat.completions.create(
                    **params, stream=False
                )

                if not response.choices or not response.choices[0].message.content:
                    raise ValueError("Empty or invalid response from LLM")

                # æ›´æ–° token è®¡æ•°
                self.update_token_count(
                    response.usage.prompt_tokens, response.usage.completion_tokens
                )

                return response.choices[0].message.content

            # æµå¼è¯·æ±‚ï¼Œå¯¹äºæµå¼ä¼ è¾“ï¼Œåœ¨å‘å‡ºè¯·æ±‚ä¹‹å‰æ›´æ–°ä¼°è®¡çš„ token è®¡æ•°
            self.update_token_count(input_tokens)

            response = await self.client.chat.completions.create(**params, stream=True)

            collected_messages = []
            completion_text = ""
            async for chunk in response:
                chunk_message = chunk.choices[0].delta.content or ""
                collected_messages.append(chunk_message)
                completion_text += chunk_message
                print(chunk_message, end="", flush=True)

            print()  # æµå¼ä¼ è¾“åçš„æ¢è¡Œ
            full_response = "".join(collected_messages).strip()
            if not full_response:
                raise ValueError("Empty response from streaming LLM")

            # ä¼°è®¡æµå¼å“åº”çš„å®Œæˆ tokens
            completion_tokens = self.count_tokens(completion_text)
            logger.info(
                f"Estimated completion tokens for streaming response: {completion_tokens}"
            )
            self.total_completion_tokens += completion_tokens

            return full_response

        except TokenLimitExceeded:
            # é‡æ–°æŠ›å‡º token é™åˆ¶é”™è¯¯è€Œä¸è®°å½•æ—¥å¿—
            raise
        except ValueError:
            logger.exception(f"Validation error")
            raise
        except OpenAIError as oe:
            logger.exception(f"OpenAI API error")
            if isinstance(oe, AuthenticationError):
                logger.error("Authentication failed. Check API key.")
            elif isinstance(oe, RateLimitError):
                logger.error("Rate limit exceeded. Consider increasing retry attempts.")
            elif isinstance(oe, APIError):
                logger.error(f"API error: {oe}")
            raise
        except Exception:
            logger.exception(f"Unexpected error in ask")
            raise

    @retry(
        wait=wait_random_exponential(min=1, max=60),
        stop=stop_after_attempt(6),
        retry=retry_if_exception_type(
            (OpenAIError, Exception, ValueError)
        ),  # Don't retry TokenLimitExceeded
    )
    async def ask_with_images(
        self,
        messages: List[Union[dict, Message]],
        images: List[Union[str, dict]],
        system_msgs: Optional[List[Union[dict, Message]]] = None,
        stream: bool = False,
        temperature: Optional[float] = None,
    ) -> str:
        """
        å‘ LLM å‘é€å¸¦æœ‰å›¾åƒçš„æç¤ºå¹¶è·å–å“åº”ã€‚

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            images: å›¾åƒ URL æˆ–å›¾åƒæ•°æ®å­—å…¸åˆ—è¡¨
            system_msgs: å¯é€‰çš„è¦å‰ç½®çš„ç³»ç»Ÿæ¶ˆæ¯
            stream (bool): æ˜¯å¦æµå¼ä¼ è¾“å“åº”
            temperature (float): å“åº”çš„é‡‡æ ·æ¸©åº¦

        Returns:
            str: ç”Ÿæˆçš„å“åº”

        Raises:
            TokenLimitExceeded: å¦‚æœè¶…è¿‡ token é™åˆ¶
            ValueError: å¦‚æœæ¶ˆæ¯æ— æ•ˆæˆ–å“åº”ä¸ºç©º
            OpenAIError: å¦‚æœ API è°ƒç”¨åœ¨é‡è¯•åå¤±è´¥
            Exception: å¯¹äºæ„å¤–é”™è¯¯
        """
        try:
            # å¯¹äº ask_with_imagesï¼Œæˆ‘ä»¬æ€»æ˜¯å°† supports_images è®¾ç½®ä¸º Trueï¼Œå› ä¸º
            # æ­¤æ–¹æ³•åº”è¯¥åªä½¿ç”¨æ”¯æŒå›¾åƒçš„æ¨¡å‹è°ƒç”¨
            if self.model not in MULTIMODAL_MODELS:
                raise ValueError(
                    f"Model {self.model} does not support images. Use a model from {MULTIMODAL_MODELS}"
                )

            # ä½¿ç”¨å›¾åƒæ”¯æŒæ ¼å¼åŒ–æ¶ˆæ¯
            formatted_messages = self.format_messages(messages, supports_images=True)

            # ç¡®ä¿æœ€åä¸€æ¡æ¶ˆæ¯æ¥è‡ªç”¨æˆ·ä»¥é™„åŠ å›¾åƒ
            if not formatted_messages or formatted_messages[-1]["role"] != "user":
                raise ValueError(
                    "The last message must be from the user to attach images"
                )

            # å¤„ç†æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä»¥åŒ…å«å›¾åƒ
            last_message = formatted_messages[-1]

            # å¦‚æœéœ€è¦ï¼Œå°†å†…å®¹è½¬æ¢ä¸ºå¤šæ¨¡æ€æ ¼å¼
            content = last_message["content"]
            multimodal_content = (
                [{"type": "text", "text": content}]
                if isinstance(content, str)
                else content
                if isinstance(content, list)
                else []
            )

            # å°†å›¾åƒæ·»åŠ åˆ°å†…å®¹ä¸­
            for image in images:
                if isinstance(image, str):
                    multimodal_content.append(
                        {"type": "image_url", "image_url": {"url": image}}
                    )
                elif isinstance(image, dict) and "url" in image:
                    multimodal_content.append({"type": "image_url", "image_url": image})
                elif isinstance(image, dict) and "image_url" in image:
                    multimodal_content.append(image)
                else:
                    raise ValueError(f"Unsupported image format: {image}")

            # ä½¿ç”¨å¤šæ¨¡æ€å†…å®¹æ›´æ–°æ¶ˆæ¯
            last_message["content"] = multimodal_content

            # å¦‚æœæä¾›äº†ç³»ç»Ÿæ¶ˆæ¯ï¼Œåˆ™æ·»åŠ å®ƒä»¬
            if system_msgs:
                all_messages = (
                    self.format_messages(system_msgs, supports_images=True)
                    + formatted_messages
                )
            else:
                all_messages = formatted_messages

            # è®¡ç®— tokens å¹¶æ£€æŸ¥é™åˆ¶
            input_tokens = self.count_message_tokens(all_messages)
            if not self.check_token_limit(input_tokens):
                raise TokenLimitExceeded(self.get_limit_error_message(input_tokens))

            # è®¾ç½® API å‚æ•°
            params = {
                "model": self.model,
                "messages": all_messages,
                "stream": stream,
            }

            # æ·»åŠ æ¨¡å‹ç‰¹å®šå‚æ•°
            if self.model in REASONING_MODELS:
                params["max_completion_tokens"] = self.max_tokens
            else:
                params["max_tokens"] = self.max_tokens
                params["temperature"] = (
                    temperature if temperature is not None else self.temperature
                )

            # å¤„ç†éæµå¼è¯·æ±‚
            if not stream:
                response = await self.client.chat.completions.create(**params)

                if not response.choices or not response.choices[0].message.content:
                    raise ValueError("Empty or invalid response from LLM")

                self.update_token_count(response.usage.prompt_tokens)
                return response.choices[0].message.content

            # å¤„ç†æµå¼è¯·æ±‚
            self.update_token_count(input_tokens)
            response = await self.client.chat.completions.create(**params)

            collected_messages = []
            async for chunk in response:
                chunk_message = chunk.choices[0].delta.content or ""
                collected_messages.append(chunk_message)
                print(chunk_message, end="", flush=True)

            print()  # æµå¼ä¼ è¾“åçš„æ¢è¡Œ
            full_response = "".join(collected_messages).strip()

            if not full_response:
                raise ValueError("Empty response from streaming LLM")

            return full_response

        except TokenLimitExceeded:
            raise
        except ValueError as ve:
            logger.error(f"Validation error in ask_with_images: {ve}")
            raise
        except OpenAIError as oe:
            logger.error(f"OpenAI API error: {oe}")
            if isinstance(oe, AuthenticationError):
                logger.error("Authentication failed. Check API key.")
            elif isinstance(oe, RateLimitError):
                logger.error("Rate limit exceeded. Consider increasing retry attempts.")
            elif isinstance(oe, APIError):
                logger.error(f"API error: {oe}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error in ask_with_images: {e}")
            raise

    @retry(
        wait=wait_random_exponential(min=1, max=60),
        stop=stop_after_attempt(6),
        retry=retry_if_exception_type(
            (OpenAIError, Exception, ValueError)
        ),  # Don't retry TokenLimitExceeded
    )
    async def ask_tool(
        self,
        messages: List[Union[dict, Message]],
        system_msgs: Optional[List[Union[dict, Message]]] = None,
        timeout: int = 300,
        tools: Optional[List[dict]] = None,
        tool_choice: TOOL_CHOICE_TYPE = ToolChoice.AUTO,  # type: ignore
        temperature: Optional[float] = None,
        **kwargs,
    ) -> ChatCompletionMessage | None:
        """
        ä½¿ç”¨å‡½æ•°/å·¥å…·è¯·æ±‚ LLM å¹¶è¿”å›å“åº”ã€‚

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            system_msgs: å¯é€‰çš„è¦å‰ç½®çš„ç³»ç»Ÿæ¶ˆæ¯
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            tools: è¦ä½¿ç”¨çš„å·¥å…·åˆ—è¡¨
            tool_choice: å·¥å…·é€‰æ‹©ç­–ç•¥
            temperature: å“åº”çš„é‡‡æ ·æ¸©åº¦
            **kwargs: é¢å¤–çš„å®Œæˆå‚æ•°

        Returns:
            ChatCompletionMessage: æ¨¡å‹çš„å“åº”

        Raises:
            TokenLimitExceeded: å¦‚æœè¶…è¿‡ token é™åˆ¶
            ValueError: å¦‚æœå·¥å…·ã€tool_choice æˆ–æ¶ˆæ¯æ— æ•ˆ
            OpenAIError: å¦‚æœ API è°ƒç”¨åœ¨é‡è¯•åå¤±è´¥
            Exception: å¯¹äºæ„å¤–é”™è¯¯
        """
        try:
            # éªŒè¯ tool_choice
            if tool_choice not in TOOL_CHOICE_VALUES:
                raise ValueError(f"Invalid tool_choice: {tool_choice}")

            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒå›¾åƒ
            supports_images = self.model in MULTIMODAL_MODELS

            # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥æ˜¯å¦æœ‰å›¾åƒè¾“å…¥
            has_images = any(
                isinstance(msg, dict) and msg.get("base64_image")
                or isinstance(msg, Message) and msg.base64_image
                for msg in (system_msgs or []) + messages
            )

            if supports_images:
                logger.info(f"ğŸ‘ï¸ Vision model enabled for tool calling: {self.model}")
                if has_images:
                    logger.info(f"ğŸ“· Image detected in tool call messages - will be sent to vision model")
            else:
                # åªæœ‰åœ¨æœ‰å›¾ç‰‡ä½†æ¨¡å‹ä¸æ”¯æŒæ—¶ï¼Œæ‰è¾“å‡ºè­¦å‘Š
                # å¦‚æœæ²¡æœ‰å›¾ç‰‡ï¼Œå°±ä¸éœ€è¦è­¦å‘Šï¼ˆæ¨¡å‹ä¸æ”¯æŒå›¾ç‰‡ä½†ä¸å½±å“æ­£å¸¸ä½¿ç”¨ï¼‰
                if has_images:
                    logger.warning(f"âš ï¸ Model {self.model} does NOT support images for tool calling")
                    logger.warning(f"âš ï¸ Images detected but will be ignored (model doesn't support vision)")

            # æ ¼å¼åŒ–æ¶ˆæ¯
            if system_msgs:
                system_msgs = self.format_messages(system_msgs, supports_images)
                messages = system_msgs + self.format_messages(messages, supports_images)
            else:
                messages = self.format_messages(messages, supports_images)

            # è®¡ç®—è¾“å…¥ token æ•°
            input_tokens = self.count_message_tokens(messages)

            # å¦‚æœæœ‰å·¥å…·ï¼Œè®¡ç®—å·¥å…·æè¿°çš„ token æ•°
            tools_tokens = 0
            if tools:
                for tool in tools:
                    tools_tokens += self.count_tokens(str(tool))

            input_tokens += tools_tokens

            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡ token é™åˆ¶
            if not self.check_token_limit(input_tokens):
                error_message = self.get_limit_error_message(input_tokens)
                # å¼•å‘ä¸€ä¸ªä¸ä¼šè¢«é‡è¯•çš„ç‰¹æ®Šå¼‚å¸¸
                raise TokenLimitExceeded(error_message)

            # å¦‚æœæä¾›äº†å·¥å…·ï¼Œåˆ™éªŒè¯å®ƒä»¬
            if tools:
                for tool in tools:
                    if not isinstance(tool, dict) or "type" not in tool:
                        raise ValueError("Each tool must be a dict with 'type' field")

            # è®¾ç½®å®Œæˆè¯·æ±‚
            params = {
                "model": self.model,
                "messages": messages,
                "tools": tools,
                "tool_choice": tool_choice,
                "timeout": timeout,
                **kwargs,
            }

            if self.model in REASONING_MODELS:
                params["max_completion_tokens"] = self.max_tokens
            else:
                params["max_tokens"] = self.max_tokens
                params["temperature"] = (
                    temperature if temperature is not None else self.temperature
                )

            params["stream"] = False  # å¯¹äºå·¥å…·è¯·æ±‚ï¼Œå§‹ç»ˆä½¿ç”¨éæµå¼ä¼ è¾“
            response: ChatCompletion = await self.client.chat.completions.create(
                **params
            )

            # æ£€æŸ¥å“åº”æ˜¯å¦æœ‰æ•ˆ
            if not response.choices or not response.choices[0].message:
                print(response)
                # raise ValueError("Invalid or empty response from LLM")
                return None

            # æ›´æ–° token è®¡æ•°
            self.update_token_count(
                response.usage.prompt_tokens, response.usage.completion_tokens
            )

            return response.choices[0].message

        except TokenLimitExceeded:
            # Re-raise token limit errors without logging
            raise
        except ValueError as ve:
            logger.error(f"Validation error in ask_tool: {ve}")
            raise
        except OpenAIError as oe:
            logger.error(f"OpenAI API error: {oe}")
            if isinstance(oe, AuthenticationError):
                logger.error("Authentication failed. Check API key.")
            elif isinstance(oe, RateLimitError):
                logger.error("Rate limit exceeded. Consider increasing retry attempts.")
            elif isinstance(oe, APIError):
                error_msg = str(oe)
                logger.error(f"API error: {error_msg}")
                # å¦‚æœæ˜¯ 404 é”™è¯¯ï¼Œæä¾›æ›´è¯¦ç»†çš„è¯Šæ–­ä¿¡æ¯
                if "404" in error_msg or "not found" in error_msg.lower():
                    logger.error(f"Model: {self.model}, Base URL: {self.base_url}")
                    logger.error("Possible issues:")
                    logger.error("1. Model name might be incorrect")
                    logger.error("2. API endpoint might be wrong")
                    logger.error("3. Model might not support tools/function calling")
            raise
        except Exception as e:
            logger.error(f"Unexpected error in ask_tool: {e}")
            raise
