"""
OpenManus çš„ Crawl4AI ç½‘é¡µçˆ¬è™«å·¥å…·

æ­¤å·¥å…·é›†æˆäº† Crawl4AIï¼Œä¸€ä¸ªä¸“ä¸º LLM å’Œ AI agent è®¾è®¡çš„é«˜æ€§èƒ½ç½‘é¡µçˆ¬è™«ï¼Œ
æä¾›å¿«é€Ÿã€ç²¾ç¡®ä¸”é€‚åˆ AI çš„æ•°æ®æå–ï¼Œå¹¶ç”Ÿæˆå¹²å‡€çš„ Markdownã€‚
"""

import asyncio
from typing import List, Union
from urllib.parse import urlparse

from app.logger import logger
from app.tool.base import BaseTool, ToolResult


class Crawl4aiTool(BaseTool):
    """
    ç”± Crawl4AI é©±åŠ¨çš„ç½‘é¡µçˆ¬è™«å·¥å…·ã€‚

    æä¾›é’ˆå¯¹ AI å¤„ç†ä¼˜åŒ–çš„å¹²å‡€ Markdown æå–ã€‚
    """

    name: str = "crawl4ai"
    description: str = """ä»ç½‘é¡µæå–å¹²å‡€ã€é€‚åˆ AI çš„å†…å®¹çš„ç½‘é¡µçˆ¬è™«ã€‚

    åŠŸèƒ½ï¼š
    - æå–é’ˆå¯¹ LLM ä¼˜åŒ–çš„å¹²å‡€ Markdown å†…å®¹
    - å¤„ç† JavaScript å¯†é›†å‹ç½‘ç«™å’ŒåŠ¨æ€å†…å®¹
    - æ”¯æŒåœ¨å•ä¸ªè¯·æ±‚ä¸­å¤„ç†å¤šä¸ª URL
    - å¿«é€Ÿå¯é ï¼Œå†…ç½®é”™è¯¯å¤„ç†

    éå¸¸é€‚åˆå†…å®¹åˆ†æã€ç ”ç©¶å’Œå°†ç½‘é¡µå†…å®¹æä¾›ç»™ AI æ¨¡å‹ã€‚"""

    parameters: dict = {
        "type": "object",
        "properties": {
            "urls": {
                "type": "array",
                "items": {"type": "string"},
                "description": "ï¼ˆå¿…éœ€ï¼‰è¦çˆ¬å–çš„ URL åˆ—è¡¨ã€‚å¯ä»¥æ˜¯å•ä¸ª URL æˆ–å¤šä¸ª URLã€‚",
                "minItems": 1,
            },
            "timeout": {
                "type": "integer",
                "description": "ï¼ˆå¯é€‰ï¼‰æ¯ä¸ª URL çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ã€‚é»˜è®¤ä¸º 30ã€‚",
                "default": 30,
                "minimum": 5,
                "maximum": 120,
            },
            "bypass_cache": {
                "type": "boolean",
                "description": "ï¼ˆå¯é€‰ï¼‰æ˜¯å¦ç»•è¿‡ç¼“å­˜å¹¶è·å–æ–°å†…å®¹ã€‚é»˜è®¤ä¸º falseã€‚",
                "default": False,
            },
            "word_count_threshold": {
                "type": "integer",
                "description": "ï¼ˆå¯é€‰ï¼‰å†…å®¹å—çš„æœ€å°å­—æ•°ã€‚é»˜è®¤ä¸º 10ã€‚",
                "default": 10,
                "minimum": 1,
            },
        },
        "required": ["urls"],
    }

    async def execute(
        self,
        urls: Union[str, List[str]],
        timeout: int = 30,
        bypass_cache: bool = False,
        word_count_threshold: int = 10,
    ) -> ToolResult:
        """
        æ‰§è¡ŒæŒ‡å®š URL çš„ç½‘é¡µçˆ¬å–ã€‚

        Args:
            urls: è¦çˆ¬å–çš„å•ä¸ª URL å­—ç¬¦ä¸²æˆ– URL åˆ—è¡¨
            timeout: æ¯ä¸ª URL çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            bypass_cache: æ˜¯å¦ç»•è¿‡ç¼“å­˜
            word_count_threshold: å†…å®¹å—çš„æœ€å°å­—æ•°

        Returns:
            åŒ…å«çˆ¬å–ç»“æœçš„ ToolResult
        """
        # å°† URL è§„èŒƒåŒ–ä¸ºåˆ—è¡¨
        if isinstance(urls, str):
            url_list = [urls]
        else:
            url_list = urls

        # éªŒè¯ URL
        valid_urls = []
        for url in url_list:
            if self._is_valid_url(url):
                valid_urls.append(url)
            else:
                logger.warning(f"Invalid URL skipped: {url}")

        if not valid_urls:
            return ToolResult(error="No valid URLs provided")

        try:
            # å¯¼å…¥ crawl4ai ç»„ä»¶
            from crawl4ai import (
                AsyncWebCrawler,
                BrowserConfig,
                CacheMode,
                CrawlerRunConfig,
            )

            # é…ç½®æµè§ˆå™¨è®¾ç½®
            browser_config = BrowserConfig(
                headless=True,
                verbose=False,
                browser_type="chromium",
                ignore_https_errors=True,
                java_script_enabled=True,
            )

            # é…ç½®çˆ¬è™«è®¾ç½®
            run_config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS if bypass_cache else CacheMode.ENABLED,
                word_count_threshold=word_count_threshold,
                process_iframes=True,
                remove_overlay_elements=True,
                excluded_tags=["script", "style"],
                page_timeout=timeout * 1000,  # è½¬æ¢ä¸ºæ¯«ç§’
                verbose=False,
                wait_until="domcontentloaded",
            )

            results = []
            successful_count = 0
            failed_count = 0

            # å¤„ç†æ¯ä¸ª URL
            async with AsyncWebCrawler(config=browser_config) as crawler:
                for url in valid_urls:
                    try:
                        logger.info(f"ğŸ•·ï¸ Crawling URL: {url}")
                        start_time = asyncio.get_event_loop().time()

                        result = await crawler.arun(url=url, config=run_config)

                        end_time = asyncio.get_event_loop().time()
                        execution_time = end_time - start_time

                        if result.success:
                            # ç»Ÿè®¡ Markdown ä¸­çš„å­—æ•°
                            word_count = 0
                            if hasattr(result, "markdown") and result.markdown:
                                word_count = len(result.markdown.split())

                            # ç»Ÿè®¡é“¾æ¥æ•°
                            links_count = 0
                            if hasattr(result, "links") and result.links:
                                internal_links = result.links.get("internal", [])
                                external_links = result.links.get("external", [])
                                links_count = len(internal_links) + len(external_links)

                            # ç»Ÿè®¡å›¾ç‰‡æ•°
                            images_count = 0
                            if hasattr(result, "media") and result.media:
                                images = result.media.get("images", [])
                                images_count = len(images)

                            results.append(
                                {
                                    "url": url,
                                    "success": True,
                                    "status_code": getattr(result, "status_code", 200),
                                    "title": result.metadata.get("title")
                                    if result.metadata
                                    else None,
                                    "markdown": result.markdown
                                    if hasattr(result, "markdown")
                                    else None,
                                    "word_count": word_count,
                                    "links_count": links_count,
                                    "images_count": images_count,
                                    "execution_time": execution_time,
                                }
                            )
                            successful_count += 1
                            logger.info(
                                f"âœ… Successfully crawled {url} in {execution_time:.2f}s"
                            )

                        else:
                            results.append(
                                {
                                    "url": url,
                                    "success": False,
                                    "error_message": getattr(
                                        result, "error_message", "Unknown error"
                                    ),
                                    "execution_time": execution_time,
                                }
                            )
                            failed_count += 1
                            logger.warning(f"âŒ Failed to crawl {url}")

                    except Exception as e:
                        error_msg = f"Error crawling {url}: {str(e)}"
                        logger.error(error_msg)
                        results.append(
                            {"url": url, "success": False, "error_message": error_msg}
                        )
                        failed_count += 1

            # æ ¼å¼åŒ–è¾“å‡º
            output_lines = [f"ğŸ•·ï¸ Crawl4AI Results Summary:"]
            output_lines.append(f"ğŸ“Š Total URLs: {len(valid_urls)}")
            output_lines.append(f"âœ… Successful: {successful_count}")
            output_lines.append(f"âŒ Failed: {failed_count}")
            output_lines.append("")

            for i, result in enumerate(results, 1):
                output_lines.append(f"{i}. {result['url']}")

                if result["success"]:
                    output_lines.append(
                        f"   âœ… Status: Success (HTTP {result.get('status_code', 'N/A')})"
                    )
                    if result.get("title"):
                        output_lines.append(f"   ğŸ“„ Title: {result['title']}")

                    if result.get("markdown"):
                        # æ˜¾ç¤º Markdown å†…å®¹çš„å‰ 300 ä¸ªå­—ç¬¦
                        content_preview = result["markdown"]
                        if len(result["markdown"]) > 300:
                            content_preview += "..."
                        output_lines.append(f"   ğŸ“ Content: {content_preview}")

                    output_lines.append(
                        f"   ğŸ“Š Stats: {result.get('word_count', 0)} words, {result.get('links_count', 0)} links, {result.get('images_count', 0)} images"
                    )

                    if result.get("execution_time"):
                        output_lines.append(
                            f"   â±ï¸ Time: {result['execution_time']:.2f}s"
                        )
                else:
                    output_lines.append(f"   âŒ Status: Failed")
                    if result.get("error_message"):
                        output_lines.append(f"   ğŸš« Error: {result['error_message']}")

                output_lines.append("")

            return ToolResult(output="\n".join(output_lines))

        except ImportError:
            error_msg = "Crawl4AI is not installed. Please install it with: pip install crawl4ai"
            logger.error(error_msg)
            return ToolResult(error=error_msg)
        except Exception as e:
            error_msg = f"Crawl4AI execution failed: {str(e)}"
            logger.error(error_msg)
            return ToolResult(error=error_msg)

    def _is_valid_url(self, url: str) -> bool:
        """éªŒè¯ URL æ ¼å¼æ˜¯å¦æ­£ç¡®ã€‚"""
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc]) and result.scheme in [
                "http",
                "https",
            ]
        except Exception:
            return False
