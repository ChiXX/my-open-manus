import asyncio
from typing import Any, Dict, List, Optional

import requests
from bs4 import BeautifulSoup
from pydantic import BaseModel, ConfigDict, Field, model_validator
from tenacity import retry, stop_after_attempt, wait_exponential

from app.config import config
from app.logger import logger
from app.tool.base import BaseTool, ToolResult
from app.tool.search import (
    BaiduSearchEngine,
    BingSearchEngine,
    DuckDuckGoSearchEngine,
    GoogleSearchEngine,
    WebSearchEngine,
)
from app.tool.search.base import SearchItem


class SearchResult(BaseModel):
    """è¡¨ç¤ºæœç´¢å¼•æ“è¿”å›çš„å•ä¸ªæœç´¢ç»“æœã€‚"""

    model_config = ConfigDict(arbitrary_types_allowed=True)

    position: int = Field(description="åœ¨æœç´¢ç»“æœä¸­çš„ä½ç½®")
    url: str = Field(description="æœç´¢ç»“æœçš„ URL")
    title: str = Field(default="", description="æœç´¢ç»“æœçš„æ ‡é¢˜")
    description: str = Field(
        default="", description="æœç´¢ç»“æœçš„æè¿°æˆ–æ‘˜è¦"
    )
    source: str = Field(description="æä¾›æ­¤ç»“æœçš„æœç´¢å¼•æ“")
    raw_content: Optional[str] = Field(
        default=None, description="å¦‚æœå¯ç”¨ï¼Œæ¥è‡ªæœç´¢ç»“æœé¡µé¢çš„åŸå§‹å†…å®¹"
    )

    def __str__(self) -> str:
        """æœç´¢ç»“æœçš„å­—ç¬¦ä¸²è¡¨ç¤ºã€‚"""
        return f"{self.title} ({self.url})"


class SearchMetadata(BaseModel):
    """å…³äºæœç´¢æ“ä½œçš„å…ƒæ•°æ®ã€‚"""

    model_config = ConfigDict(arbitrary_types_allowed=True)

    total_results: int = Field(description="æ‰¾åˆ°çš„ç»“æœæ€»æ•°")
    language: str = Field(description="ç”¨äºæœç´¢çš„è¯­è¨€ä»£ç ")
    country: str = Field(description="ç”¨äºæœç´¢çš„å›½å®¶ä»£ç ")


class SearchResponse(ToolResult):
    """æ¥è‡ªç½‘é¡µæœç´¢å·¥å…·çš„ç»“æ„åŒ–å“åº”ï¼Œç»§æ‰¿è‡ª ToolResultã€‚"""

    query: str = Field(description="æ‰§è¡Œçš„æœç´¢æŸ¥è¯¢")
    results: List[SearchResult] = Field(
        default_factory=list, description="æœç´¢ç»“æœåˆ—è¡¨"
    )
    metadata: Optional[SearchMetadata] = Field(
        default=None, description="å…³äºæœç´¢çš„å…ƒæ•°æ®"
    )

    @model_validator(mode="after")
    def populate_output(self) -> "SearchResponse":
        """æ ¹æ®æœç´¢ç»“æœå¡«å……è¾“å‡ºæˆ–é”™è¯¯å­—æ®µã€‚"""
        if self.error:
            return self

        result_text = [f"Search results for '{self.query}':"]

        for i, result in enumerate(self.results, 1):
            # Add title with position number
            title = result.title.strip() or "No title"
            result_text.append(f"\n{i}. {title}")

            # Add URL with proper indentation
            result_text.append(f"   URL: {result.url}")

            # Add description if available
            if result.description.strip():
                result_text.append(f"   Description: {result.description}")

            # Add content preview if available
            if result.raw_content:
                content_preview = result.raw_content[:1000].replace("\n", " ").strip()
                if len(result.raw_content) > 1000:
                    content_preview += "..."
                result_text.append(f"   Content: {content_preview}")

        # Add metadata at the bottom if available
        if self.metadata:
            result_text.extend(
                [
                    f"\nMetadata:",
                    f"- Total results: {self.metadata.total_results}",
                    f"- Language: {self.metadata.language}",
                    f"- Country: {self.metadata.country}",
                ]
            )

        self.output = "\n".join(result_text)
        return self


class WebContentFetcher:
    """ç”¨äºè·å–ç½‘é¡µå†…å®¹çš„å·¥å…·ç±»ã€‚"""

    @staticmethod
    async def fetch_content(url: str, timeout: int = 10) -> Optional[str]:
        """
        ä»ç½‘é¡µè·å–å¹¶æå–ä¸»è¦å†…å®¹ã€‚

        Args:
            url: è¦è·å–å†…å®¹çš„ URL
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            æå–çš„æ–‡æœ¬å†…å®¹ï¼Œå¦‚æœè·å–å¤±è´¥åˆ™è¿”å› None
        """
        headers = {
            "WebSearch": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

        try:
            # ä½¿ç”¨ asyncio åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œ requests
            response = await asyncio.get_event_loop().run_in_executor(
                None, lambda: requests.get(url, headers=headers, timeout=timeout)
            )

            if response.status_code != 200:
                logger.warning(
                    f"Failed to fetch content from {url}: HTTP {response.status_code}"
                )
                return None

            # ä½¿ç”¨ BeautifulSoup è§£æ HTML
            soup = BeautifulSoup(response.text, "html.parser")

            # åˆ é™¤ script å’Œ style å…ƒç´ 
            for script in soup(["script", "style", "header", "footer", "nav"]):
                script.extract()

            # è·å–æ–‡æœ¬å†…å®¹
            text = soup.get_text(separator="\n", strip=True)

            # æ¸…ç†ç©ºç™½å¹¶é™åˆ¶å¤§å°ï¼ˆæœ€å¤§ 100KBï¼‰
            text = " ".join(text.split())
            return text[:10000] if text else None

        except Exception as e:
            logger.warning(f"Error fetching content from {url}: {e}")
            return None


class WebSearch(BaseTool):
    """ä½¿ç”¨å„ç§æœç´¢å¼•æ“æœç´¢ç½‘é¡µä¿¡æ¯ã€‚"""

    name: str = "web_search"
    description: str = """æœç´¢ç½‘é¡µä»¥è·å–å…³äºä»»ä½•ä¸»é¢˜çš„å®æ—¶ä¿¡æ¯ã€‚
    æ­¤å·¥å…·è¿”å›åŒ…å«ç›¸å…³ä¿¡æ¯ã€URLã€æ ‡é¢˜å’Œæè¿°çš„å…¨é¢æœç´¢ç»“æœã€‚
    å¦‚æœä¸»è¦æœç´¢å¼•æ“å¤±è´¥ï¼Œå®ƒä¼šè‡ªåŠ¨å›é€€åˆ°å¤‡ç”¨å¼•æ“ã€‚"""
    parameters: dict = {
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "ï¼ˆå¿…éœ€ï¼‰è¦æäº¤ç»™æœç´¢å¼•æ“çš„æœç´¢æŸ¥è¯¢ã€‚",
            },
            "num_results": {
                "type": "integer",
                "description": "ï¼ˆå¯é€‰ï¼‰è¦è¿”å›çš„æœç´¢ç»“æœæ•°é‡ã€‚é»˜è®¤ä¸º 5ã€‚",
                "default": 5,
            },
            "lang": {
                "type": "string",
                "description": "ï¼ˆå¯é€‰ï¼‰æœç´¢ç»“æœçš„è¯­è¨€ä»£ç ï¼ˆé»˜è®¤: enï¼‰ã€‚",
                "default": "en",
            },
            "country": {
                "type": "string",
                "description": "ï¼ˆå¯é€‰ï¼‰æœç´¢ç»“æœçš„å›½å®¶ä»£ç ï¼ˆé»˜è®¤: usï¼‰ã€‚",
                "default": "us",
            },
            "fetch_content": {
                "type": "boolean",
                "description": "ï¼ˆå¯é€‰ï¼‰æ˜¯å¦ä»ç»“æœé¡µé¢è·å–å®Œæ•´å†…å®¹ã€‚é»˜è®¤ä¸º falseã€‚",
                "default": False,
            },
        },
        "required": ["query"],
    }
    _search_engine: dict[str, WebSearchEngine] = {
        "google": GoogleSearchEngine(),
        "baidu": BaiduSearchEngine(),
        "duckduckgo": DuckDuckGoSearchEngine(),
        "bing": BingSearchEngine(),
    }
    content_fetcher: WebContentFetcher = WebContentFetcher()

    async def execute(
        self,
        query: str,
        num_results: int = 5,
        lang: Optional[str] = None,
        country: Optional[str] = None,
        fetch_content: bool = False,
    ) -> SearchResponse:
        """
        æ‰§è¡Œç½‘é¡µæœç´¢å¹¶è¿”å›è¯¦ç»†çš„æœç´¢ç»“æœã€‚

        Args:
            query: è¦æäº¤ç»™æœç´¢å¼•æ“çš„æœç´¢æŸ¥è¯¢
            num_results: è¦è¿”å›çš„æœç´¢ç»“æœæ•°é‡ï¼ˆé»˜è®¤: 5ï¼‰
            lang: æœç´¢ç»“æœçš„è¯­è¨€ä»£ç ï¼ˆé»˜è®¤æ¥è‡ªé…ç½®ï¼‰
            country: æœç´¢ç»“æœçš„å›½å®¶ä»£ç ï¼ˆé»˜è®¤æ¥è‡ªé…ç½®ï¼‰
            fetch_content: æ˜¯å¦ä»ç»“æœé¡µé¢è·å–å†…å®¹ï¼ˆé»˜è®¤: Falseï¼‰

        Returns:
            åŒ…å«æœç´¢ç»“æœå’Œå…ƒæ•°æ®çš„ç»“æ„åŒ–å“åº”
        """
        # ä»é…ç½®è·å–è®¾ç½®
        retry_delay = (
            getattr(config.search_config, "retry_delay", 60)
            if config.search_config
            else 60
        )
        max_retries = (
            getattr(config.search_config, "max_retries", 3)
            if config.search_config
            else 3
        )

        # å¦‚æœæœªæŒ‡å®šï¼Œä½¿ç”¨é…ç½®ä¸­çš„ lang å’Œ country å€¼
        if lang is None:
            lang = (
                getattr(config.search_config, "lang", "en")
                if config.search_config
                else "en"
            )

        if country is None:
            country = (
                getattr(config.search_config, "country", "us")
                if config.search_config
                else "us"
            )

        search_params = {"lang": lang, "country": country}

        # å½“æ‰€æœ‰å¼•æ“éƒ½å¤±è´¥æ—¶ï¼Œå°è¯•é‡è¯•æœç´¢
        for retry_count in range(max_retries + 1):
            results = await self._try_all_engines(query, num_results, search_params)

            if results:
                # å¦‚æœè¯·æ±‚ï¼Œåˆ™è·å–å†…å®¹
                if fetch_content:
                    results = await self._fetch_content_for_results(results)

                # è¿”å›æˆåŠŸçš„ç»“æ„åŒ–å“åº”
                return SearchResponse(
                    status="success",
                    query=query,
                    results=results,
                    metadata=SearchMetadata(
                        total_results=len(results),
                        language=lang,
                        country=country,
                    ),
                )

            if retry_count < max_retries:
                # æ‰€æœ‰å¼•æ“éƒ½å¤±è´¥ï¼Œç­‰å¾…å¹¶é‡è¯•
                logger.warning(
                    f"All search engines failed. Waiting {retry_delay} seconds before retry {retry_count + 1}/{max_retries}..."
                )
                await asyncio.sleep(retry_delay)
            else:
                logger.error(
                    f"All search engines failed after {max_retries} retries. Giving up."
                )

        # è¿”å›é”™è¯¯å“åº”
        return SearchResponse(
            query=query,
            error="All search engines failed to return results after multiple retries.",
            results=[],
        )

    async def _try_all_engines(
        self, query: str, num_results: int, search_params: Dict[str, Any]
    ) -> List[SearchResult]:
        """æŒ‰é…ç½®çš„é¡ºåºå°è¯•æ‰€æœ‰æœç´¢å¼•æ“ã€‚"""
        engine_order = self._get_engine_order()
        failed_engines = []

        for engine_name in engine_order:
            engine = self._search_engine[engine_name]
            logger.info(f"ğŸ” Attempting search with {engine_name.capitalize()}...")
            search_items = await self._perform_search_with_engine(
                engine, query, num_results, search_params
            )

            if not search_items:
                continue

            if failed_engines:
                logger.info(
                    f"Search successful with {engine_name.capitalize()} after trying: {', '.join(failed_engines)}"
                )

            # å°†æœç´¢é¡¹è½¬æ¢ä¸ºç»“æ„åŒ–ç»“æœ
            return [
                SearchResult(
                    position=i + 1,
                    url=item.url,
                    title=item.title
                    or f"Result {i+1}",  # ç¡®ä¿æˆ‘ä»¬å§‹ç»ˆæœ‰ä¸€ä¸ªæ ‡é¢˜
                    description=item.description or "",
                    source=engine_name,
                )
                for i, item in enumerate(search_items)
            ]

        if failed_engines:
            logger.error(f"All search engines failed: {', '.join(failed_engines)}")
        return []

    async def _fetch_content_for_results(
        self, results: List[SearchResult]
    ) -> List[SearchResult]:
        """è·å–ç½‘é¡µå†…å®¹å¹¶å°†å…¶æ·»åŠ åˆ°æœç´¢ç»“æœä¸­ã€‚"""
        if not results:
            return []

        # ä¸ºæ¯ä¸ªç»“æœåˆ›å»ºä»»åŠ¡
        tasks = [self._fetch_single_result_content(result) for result in results]

        # ç±»å‹æ³¨é‡Šä»¥å¸®åŠ©ç±»å‹æ£€æŸ¥å™¨
        fetched_results = await asyncio.gather(*tasks)

        # æ˜¾å¼éªŒè¯è¿”å›ç±»å‹
        return [
            (
                result
                if isinstance(result, SearchResult)
                else SearchResult(**result.dict())
            )
            for result in fetched_results
        ]

    async def _fetch_single_result_content(self, result: SearchResult) -> SearchResult:
        """è·å–å•ä¸ªæœç´¢ç»“æœçš„å†…å®¹ã€‚"""
        if result.url:
            content = await self.content_fetcher.fetch_content(result.url)
            if content:
                result.raw_content = content
        return result

    def _get_engine_order(self) -> List[str]:
        """ç¡®å®šå°è¯•æœç´¢å¼•æ“çš„é¡ºåºã€‚"""
        preferred = (
            getattr(config.search_config, "engine", "google").lower()
            if config.search_config
            else "google"
        )
        fallbacks = (
            [engine.lower() for engine in config.search_config.fallback_engines]
            if config.search_config
            and hasattr(config.search_config, "fallback_engines")
            else []
        )

        # ä»é¦–é€‰å¼•æ“å¼€å§‹ï¼Œç„¶åæ˜¯å¤‡ç”¨å¼•æ“ï¼Œæœ€åæ˜¯å‰©ä½™çš„å¼•æ“
        engine_order = [preferred] if preferred in self._search_engine else []
        engine_order.extend(
            [
                fb
                for fb in fallbacks
                if fb in self._search_engine and fb not in engine_order
            ]
        )
        engine_order.extend([e for e in self._search_engine if e not in engine_order])

        return engine_order

    @retry(
        stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=10)
    )
    async def _perform_search_with_engine(
        self,
        engine: WebSearchEngine,
        query: str,
        num_results: int,
        search_params: Dict[str, Any],
    ) -> List[SearchItem]:
        """ä½¿ç”¨ç»™å®šçš„å¼•æ“å’Œå‚æ•°æ‰§è¡Œæœç´¢ã€‚"""
        return await asyncio.get_event_loop().run_in_executor(
            None,
            lambda: list(
                engine.perform_search(
                    query,
                    num_results=num_results,
                    lang=search_params.get("lang"),
                    country=search_params.get("country"),
                )
            ),
        )


if __name__ == "__main__":
    web_search = WebSearch()
    search_response = asyncio.run(
        web_search.execute(
            query="Python programming", fetch_content=True, num_results=1
        )
    )
    print(search_response.to_tool_result())
